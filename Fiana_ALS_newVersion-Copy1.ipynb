{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f01185-3744-40aa-a125-7288504a5de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:32:48.602183Z",
     "iopub.status.busy": "2025-11-11T05:32:48.601066Z",
     "iopub.status.idle": "2025-11-11T05:32:50.431168Z",
     "shell.execute_reply": "2025-11-11T05:32:50.430228Z",
     "shell.execute_reply.started": "2025-11-11T05:32:48.602121Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17daf6c-be8a-4f96-a3f4-3cf2ca44c79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:32:50.432729Z",
     "iopub.status.busy": "2025-11-11T05:32:50.432388Z",
     "iopub.status.idle": "2025-11-11T05:32:50.439387Z",
     "shell.execute_reply": "2025-11-11T05:32:50.437768Z",
     "shell.execute_reply.started": "2025-11-11T05:32:50.432707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: ./als_spark_checkpoints\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./als_spark_checkpoints\"\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)   # Create folder\n",
    "    print(f\"Folder created: {folder_path}\")\n",
    "else:\n",
    "    print(f\"Folder already exists: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560032f5-5b1a-4951-bced-bb25014be8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:32:50.441586Z",
     "iopub.status.busy": "2025-11-11T05:32:50.440991Z",
     "iopub.status.idle": "2025-11-11T05:32:56.387163Z",
     "shell.execute_reply": "2025-11-11T05:32:56.385314Z",
     "shell.execute_reply.started": "2025-11-11T05:32:50.441531Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "     .builder\n",
    "     .master('local[*]') # tells you master is 1 laptop using all 4 executors\n",
    "     .config(\"spark.driver.memory\", \"8g\")\n",
    "     .config(\"spark.executor.memory\", \"8g\")\n",
    "     .config(\"spark.sql.shuffle.partitions\", \"8\")  # reduce for local\n",
    "     .getOrCreate()) # make new or get latest session\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"./als_spark_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91af199-9145-4e22-bcc6-ea0fad56df3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:32:56.390489Z",
     "iopub.status.busy": "2025-11-11T05:32:56.389765Z",
     "iopub.status.idle": "2025-11-11T05:32:59.400810Z",
     "shell.execute_reply": "2025-11-11T05:32:59.399120Z",
     "shell.execute_reply.started": "2025-11-11T05:32:56.390426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read board game geek file on spark\n",
    "schema = \"\"\"\n",
    "_c0 INT,\n",
    "user STRING,\n",
    "rating FLOAT,\n",
    "comment STRING,\n",
    "id INT, \n",
    "name STRING\n",
    "\"\"\"\n",
    "# Fix quote handling for comments column \n",
    "df_spark = spark.read.csv(\n",
    "    \"/mnt/data/public/bgg/bgg-19m-reviews.csv\",\n",
    "    sep=',', header=True,\n",
    "    schema=schema,\n",
    "    multiLine=True,\n",
    "    quote='\"',\n",
    "    escape='\"')\n",
    "df_spark = df_spark.drop(\"_c0\", \"comment\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b8befb-56a1-4115-8087-191e58c653c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:32:59.406456Z",
     "iopub.status.busy": "2025-11-11T05:32:59.405198Z",
     "iopub.status.idle": "2025-11-11T05:33:33.388460Z",
     "shell.execute_reply": "2025-11-11T05:33:33.386803Z",
     "shell.execute_reply.started": "2025-11-11T05:32:59.406388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map user name to integer\n",
    "user_indexer = StringIndexer(inputCol=\"user\", outputCol=\"user_id\")\n",
    "df_spark_indexed = user_indexer.fit(df_spark).transform(df_spark)\n",
    "\n",
    "# Save Spark DF mapping of user to User ID\n",
    "user_mapping = df_spark_indexed.select(\"user\", \"user_id\").distinct()\n",
    "df_spark_indexed = df_spark_indexed.drop(\"user\")\n",
    "\n",
    "# Change item column name for unformity\n",
    "df_spark_indexed = df_spark_indexed.withColumnRenamed(\"id\", \"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc361b6b-b718-42f8-96b2-9e7666dd1fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:33:33.390675Z",
     "iopub.status.busy": "2025-11-11T05:33:33.390070Z",
     "iopub.status.idle": "2025-11-11T05:33:34.914517Z",
     "shell.execute_reply": "2025-11-11T05:33:34.913163Z",
     "shell.execute_reply.started": "2025-11-11T05:33:33.390619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+\n",
      "|rating|item_id| user_id|\n",
      "+------+-------+--------+\n",
      "|  10.0|  30549|   201.0|\n",
      "|  10.0|  30549|  6591.0|\n",
      "|  10.0|  30549|   631.0|\n",
      "|  10.0|  30549|  1705.0|\n",
      "|  10.0|  30549|  5796.0|\n",
      "|  10.0|  30549|    78.0|\n",
      "|  10.0|  30549|393225.0|\n",
      "|  10.0|  30549|233206.0|\n",
      "|  10.0|  30549| 22517.0|\n",
      "|  10.0|  30549| 87298.0|\n",
      "+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_indexed.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4fe7ae-e982-4e95-a7e5-cb6f1fce9503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:33:34.917307Z",
     "iopub.status.busy": "2025-11-11T05:33:34.916235Z",
     "iopub.status.idle": "2025-11-11T05:33:54.211544Z",
     "shell.execute_reply": "2025-11-11T05:33:54.209770Z",
     "shell.execute_reply.started": "2025-11-11T05:33:34.917247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|rating|item_id|user_id|\n",
      "+------+-------+-------+\n",
      "|     0|      0|      0|\n",
      "+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count nulls per column\n",
    "null_counts = df_spark_indexed.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_spark_indexed.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfe0956-aca8-4659-a0b5-adcba5f2e00f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:33:54.214143Z",
     "iopub.status.busy": "2025-11-11T05:33:54.213479Z",
     "iopub.status.idle": "2025-11-11T05:33:54.224927Z",
     "shell.execute_reply": "2025-11-11T05:33:54.223206Z",
     "shell.execute_reply.started": "2025-11-11T05:33:54.214086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/home2/bsdsba2027/rvelasco/BDCC Labs/BDCC_Lab1/als_spark_checkpoints/c047da02-90f8-49fc-bcda-141643289c95\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getCheckpointDir())  # checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3650a1a7-d611-4738-8a41-3806e6c1fa43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:33:54.253356Z",
     "iopub.status.busy": "2025-11-11T05:33:54.253135Z",
     "iopub.status.idle": "2025-11-11T05:36:43.056466Z",
     "shell.execute_reply": "2025-11-11T05:36:43.054710Z",
     "shell.execute_reply.started": "2025-11-11T05:33:54.253334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train an ALS model\n",
    "train, test = df_spark_indexed.randomSplit([0.8, 0.2])\n",
    "als = ALS(rank=2, maxIter=5, \n",
    "          userCol=\"user_id\", itemCol='item_id', \n",
    "          ratingCol=\"rating\", coldStartStrategy='drop',\n",
    "          checkpointInterval=10  # Saves to disk after n iterations\n",
    "         )\n",
    "als_model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5a06e0-a970-4c9d-adda-c17fabbbdf9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:36:43.062207Z",
     "iopub.status.busy": "2025-11-11T05:36:43.061226Z",
     "iopub.status.idle": "2025-11-11T05:37:43.303025Z",
     "shell.execute_reply": "2025-11-11T05:37:43.301913Z",
     "shell.execute_reply.started": "2025-11-11T05:36:43.062146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error:1.2451375844400066\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = als_model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00709ded-1a7b-4852-b38d-0715f94243c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T17:14:25.630205Z",
     "iopub.status.busy": "2025-11-10T17:14:25.629455Z",
     "iopub.status.idle": "2025-11-10T17:14:25.937138Z",
     "shell.execute_reply": "2025-11-10T17:14:25.936134Z",
     "shell.execute_reply.started": "2025-11-10T17:14:25.630145Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ratings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m movieRecs \u001b[38;5;241m=\u001b[39m als_model\u001b[38;5;241m.\u001b[39mrecommendForAllItems(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate top 10 movie recommendations for a specified set of users\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m users \u001b[38;5;241m=\u001b[39m \u001b[43mratings\u001b[49m\u001b[38;5;241m.\u001b[39mselect(als\u001b[38;5;241m.\u001b[39mgetUserCol())\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      8\u001b[0m userSubsetRecs \u001b[38;5;241m=\u001b[39m als_model\u001b[38;5;241m.\u001b[39mrecommendForUserSubset(users, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate top 10 user recommendations for a specified set of movies\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ratings' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = als_model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = als_model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = als_model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = als_model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1486d0ac-1e4f-4482-ad24-a2dee26c5066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T17:15:31.451138Z",
     "iopub.status.busy": "2025-11-10T17:15:31.450410Z",
     "iopub.status.idle": "2025-11-10T17:18:06.680040Z",
     "shell.execute_reply": "2025-11-10T17:18:06.679187Z",
     "shell.execute_reply.started": "2025-11-10T17:15:31.451077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>[(149705, 8.133953094482422), (254632, 8.13036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>[(345976, 7.779404163360596), (63170, 7.649724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>[(345976, 8.195059776306152), (277538, 8.04972...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    recommendations\n",
       "0       12  [(149705, 8.133953094482422), (254632, 8.13036...\n",
       "1       13  [(345976, 7.779404163360596), (63170, 7.649724...\n",
       "2       14  [(345976, 8.195059776306152), (277538, 8.04972..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRecs.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d5ad36-b644-4898-956b-50ca6c51d458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:39:01.870391Z",
     "iopub.status.busy": "2025-11-11T05:39:01.869642Z",
     "iopub.status.idle": "2025-11-11T05:39:02.055737Z",
     "shell.execute_reply": "2025-11-11T05:39:02.054718Z",
     "shell.execute_reply.started": "2025-11-11T05:39:01.870329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations...\n",
      "Evaluation DataFrame ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, col\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "\n",
    "# Use the efficient recommendForAllUsers instead of cross join\n",
    "print(\"Generating recommendations...\")\n",
    "k = 10  # Top 10 recommendations per user\n",
    "userRecs = als_model.recommendForAllUsers(k)\n",
    "\n",
    "# Extract predicted item IDs\n",
    "dfs_preds_grouped = userRecs.select(\n",
    "    col('user_id'),\n",
    "    col('recommendations.item_id').alias('predicted_item_id_arr')\n",
    ").withColumn(\n",
    "    'predicted_item_id_arr',\n",
    "    col('predicted_item_id_arr').cast('array<double>')\n",
    ")\n",
    "\n",
    "# Get actual highly-rated items from test set\n",
    "thresh = 4.0\n",
    "test_thresh_grouped = test.filter(\n",
    "    col('rating') >= thresh\n",
    ").groupBy('user_id').agg(\n",
    "    collect_list(col('item_id').cast('double')).alias('rated_item_id_arr')\n",
    ")\n",
    "\n",
    "# Join predictions with actuals\n",
    "dfs_preds_thresh_for_eval = test_thresh_grouped.join(\n",
    "    dfs_preds_grouped, \n",
    "    on='user_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(\"Evaluation DataFrame ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76536cfa-10bd-4a03-b800-65ca86f05baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T05:39:05.882202Z",
     "iopub.status.busy": "2025-11-11T05:39:05.880971Z",
     "iopub.status.idle": "2025-11-11T05:43:52.294353Z",
     "shell.execute_reply": "2025-11-11T05:43:52.292942Z",
     "shell.execute_reply.started": "2025-11-11T05:39:05.882138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 283,372\n",
      "NDCG at k=3: 0.00010578934525071998\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, col, explode\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "\n",
    "# Step 1: Get predictions using the efficient built-in method\n",
    "# This returns top N recommendations per user (much smaller dataset!)\n",
    "k = 10  # or however many recommendations you want\n",
    "userRecs = als_model.recommendForAllUsers(k)\n",
    "\n",
    "# Step 2: Extract item IDs from recommendations\n",
    "# userRecs has format: user_id | recommendations (array of struct(item_id, rating))\n",
    "dfs_preds_grouped = userRecs.select(\n",
    "    col('user_id'),\n",
    "    col('recommendations.item_id').alias('predicted_item_id_arr')\n",
    ").withColumn(\n",
    "    'predicted_item_id_arr',\n",
    "    col('predicted_item_id_arr').cast('array<double>')\n",
    ")\n",
    "\n",
    "# Step 3: Get actual highly-rated items from test set\n",
    "thresh = 4.0\n",
    "test_thresh_grouped = test.filter(\n",
    "    col('rating') >= thresh\n",
    ").groupBy('user_id').agg(\n",
    "    collect_list(col('item_id').cast('double')).alias('rated_item_id_arr')\n",
    ")\n",
    "\n",
    "# Step 4: Join predictions with actuals\n",
    "dfs_preds_thresh_for_eval = test_thresh_grouped.join(\n",
    "    dfs_preds_grouped, \n",
    "    on='user_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 5: Check size (should be manageable now!)\n",
    "row_count = dfs_preds_thresh_for_eval.count()\n",
    "print(f\"Number of rows: {row_count:,}\")\n",
    "\n",
    "# Step 6: Evaluate\n",
    "evaluator = RankingEvaluator(\n",
    "    labelCol='rated_item_id_arr',\n",
    "    predictionCol='predicted_item_id_arr',\n",
    "    metricName='ndcgAtK',\n",
    "    k=3\n",
    ")\n",
    "ndcg_k = evaluator.evaluate(dfs_preds_thresh_for_eval)\n",
    "print(f\"NDCG at k=3: {ndcg_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d9ff1-f705-48e1-adbe-1c89fc0f5372",
   "metadata": {},
   "source": [
    "- https://medium.com/@sinha.raunak/recommendation-systems-pyspark-als-model-evaluation-rmse-map-k-recall-k-ndcg-k-477bf6df893e\n",
    "\n",
    "- https://github.com/CGrannan/building-boardgame-recommendation-systems/blob/master/spark_als_recommendation.ipynb (but no ndcg@k)\n",
    "\n",
    "fix the code below tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095ab6c1-7ebc-43d4-88aa-84bca47705b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:11:12.724669Z",
     "iopub.status.busy": "2025-11-11T04:11:12.723886Z",
     "iopub.status.idle": "2025-11-11T04:11:13.129613Z",
     "shell.execute_reply": "2025-11-11T04:11:13.128180Z",
     "shell.execute_reply.started": "2025-11-11T04:11:12.724610Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get rated movie array\n",
    "\n",
    "from pyspark.sql.functions import collect_list, col, row_number, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "\n",
    "# extract best performing model \n",
    "#best_model = cv_model.bestModel\n",
    "best_model = als_model.copy()\n",
    "\n",
    "# cross join users & movies to create a list \n",
    "# of all users and movie combinations\n",
    "all_users = train.select('user_id').distinct()\n",
    "all_movies = train.select('item_id').distinct()\n",
    "users_x_movies = all_users.crossJoin(all_movies)\n",
    "\n",
    "# get predictions\n",
    "dfs_preds = best_model.transform(users_x_movies)\n",
    "\n",
    "# join preds and train dataset to get all preds & ratings\n",
    "# the unrated user x movie pairs will be NULL \n",
    "dfs_preds_and_ratings = dfs_preds.alias('preds').join(\n",
    "    train.alias('train'),\n",
    "    (dfs_preds['user_id']==train['user_id']) & \n",
    "    (dfs_preds['item_id']==train['item_id']),\n",
    "    how='outer')\n",
    "\n",
    "# filter out the \"seen\" movies from prediction\n",
    "# get preds for unrated user x movie pairs \n",
    "# using rating col which will contain NULLs\n",
    "dfs_preds_final = dfs_preds_and_ratings.filter(\n",
    "    col('train.rating').isNull()\n",
    ").select('preds.user_id', 'preds.item_id', 'preds.prediction')\n",
    "\n",
    "# threshold for filtering predicted ratings & actual ratings\n",
    "thresh = 4.0 \n",
    "\n",
    "# filter predictions using threshold\n",
    "# rank order the predictions by predicted ratings\n",
    "dfs_preds_thresh_ranked = dfs_preds_final.filter(\n",
    "    col('prediction') >= thresh\n",
    "    ).orderBy('user_id', col('prediction').desc())\n",
    "dfs_preds_thresh_ranked_grouped = dfs_preds_thresh_ranked.groupBy('user_id').agg(\n",
    "    collect_list(col('item_id').cast('double')).alias('predicted_item_id_arr')\n",
    "    )\n",
    "\n",
    "# filter test dataset using threshold\n",
    "# rank order the test dataset by predicted ratings\n",
    "test_thresh_ranked = test.filter(\n",
    "    col('rating') >= thresh\n",
    "    ).orderBy('user_id', col('rating').desc())\n",
    "test_thresh_ranked_grouped = test_thresh_ranked.groupBy('user_id').agg(\n",
    "    collect_list(col('item_id').cast('double')).alias('rated_item_id_arr')\n",
    ")\n",
    "\n",
    "# inner join ranked test dataset with predictions \n",
    "# for every user to get two columns per user \n",
    "dfs_preds_thresh_for_eval = test_thresh_ranked_grouped.join(dfs_preds_thresh_ranked_grouped, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce374bd6-e7b6-46b4-a7ba-ecc3f68d7cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:11:14.453993Z",
     "iopub.status.busy": "2025-11-11T04:11:14.453014Z",
     "iopub.status.idle": "2025-11-11T04:25:46.272502Z",
     "shell.execute_reply": "2025-11-11T04:25:46.271247Z",
     "shell.execute_reply.started": "2025-11-11T04:11:14.453929Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m row_count \u001b[38;5;241m=\u001b[39m \u001b[43mdfs_preds_thresh_for_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/classic/dataframe.py:443\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "row_count = dfs_preds_thresh_for_eval.count()\n",
    "print(f\"Number of rows: {row_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb29329-8319-4980-8fad-bef03fa4816e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:48:59.405123Z",
     "iopub.status.busy": "2025-11-11T02:48:59.403632Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = RankingEvaluator(\n",
    "    labelCol='rated_item_id_arr', \n",
    "    predictionCol='predicted_item_id_arr', \n",
    "    metricName='ndcgAtK', \n",
    "    k=3\n",
    ")\n",
    "ndcg_k = evaluator.evaluate(dfs_preds_thresh_for_eval)\n",
    "print(f\"NDCG at k={k} : {ndcg_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e5e84-64c9-4f9c-b585-48edae4d9e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
